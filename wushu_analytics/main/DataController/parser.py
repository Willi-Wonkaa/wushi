import os
import time
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import ssl
from datetime import datetime
from .dataWriter import write_competitions
 
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
BASE_URL = "https://wushujudges.ru"
HEADERS = {"User-Agent": "Mozilla/5.0"}
MAX_RETRIES = 3
SLEEP_TIME = 0.5
 
 
# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
ssl._create_default_https_context = ssl._create_unverified_context
 
 
def fetch_page(url):
    print('fetching page: ', url)    
    response = requests.get(url, headers=HEADERS, verify=False)
    print('Page fetched')
    return response.text


def convert_date(date_str):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç—É –∏–∑ DD.MM.YYYY –≤ YYYY-MM-DD"""
    try:
        return datetime.strptime(date_str, '%d.%m.%Y').strftime('%Y-%m-%d')
    except ValueError:
        return None
 
 
def parse_competitions():
    """–ü–∞—Ä—Å–∏—Ç —Å–ø–∏—Å–æ–∫ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    url = BASE_URL
    print(f'Parsing competitions from: {url}')
 
    html = fetch_page(url)
    if not html:
        print("Failed to fetch page")
        return []
 
    soup = BeautifulSoup(html, "html.parser")
    competitions = []
 
    # –ù–∞—Ö–æ–¥–∏–º —Ç–∞–±–ª–∏—Ü—É —Å —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è–º–∏
    table = soup.find("table", class_="table")
    if not table:
        print("Table not found")
        return []
 
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–∏ tbody
    tbody = table.find("tbody")
    if not tbody:
        print("Tbody not found")
        return []
 
    rows = tbody.find_all("tr")
 
    for row in rows:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        if "datatable__empty" in row.get("class", []):
            continue
 
        cells = row.find_all("td")
        if len(cells) < 4:
            continue
 
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        name_cell = cells[0].find("a")
        if not name_cell:
            continue
 
        name = name_cell.text.strip()
        link = name_cell.get("href", "")
        full_link = BASE_URL + link if link else ""
 
        city = cells[1].text.strip()
        start_date = convert_date(cells[2].text.strip())
        end_date = convert_date(cells[3].text.strip())
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –¥–∞—Ç—ã –Ω–µ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å
        if not start_date or not end_date:
            continue
 
        competitions.append({
            "name": name,
            "city": city,
            "start_date": start_date,
            "end_date": end_date,
            "link": full_link
        })
 
    print(f"Found {len(competitions)} competitions:")
    for comp in competitions:
        print(f"- {comp['name']} ({comp['city']}) {comp['start_date']}-{comp['end_date']}")
 
    return competitions
 
 
def parse_competition_detail(competition_url):
    """–ü–∞—Ä—Å–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏"""
    print(f'Parsing competition detail from: {competition_url}')
    
    html = fetch_page(competition_url)
    if not html:
        print("Failed to fetch competition page")
        return None
    
    soup = BeautifulSoup(html, "html.parser")
    
    # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è
    title_tag = soup.find("h1") or soup.find("title")
    competition_name = title_tag.text.strip().split(" | ")[-1] if title_tag else "Unknown"
    
    # –ò—â–µ–º —Ä–µ–≥–ª–∞–º–µ–Ω—Ç
    regulation = ""
    regulation_block = soup.find("div", class_="regulation") or soup.find("div", {"id": "regulation"})
    if regulation_block:
        regulation = regulation_block.get_text(strip=True)
    
    # –ü–∞—Ä—Å–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ –∫–æ–≤—Ä–∞–º
    categories = []
    current_time = datetime.now()
    
    # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏ —Å–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –∏—Ö –±–µ–∑ —Å—Ç–∞—Ç—É—Å–æ–≤
    temp_categories = []
    
    # –ò—â–µ–º –≤—Å–µ –±–ª–æ–∫–∏ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    for category_block in soup.find_all("div", class_="d-flex"):
        category_name_tag = category_block.find("h3")
        if not category_name_tag:
            continue
        
        raw_category = category_name_tag.text.strip().replace("\n", " ")
        time_range = category_block.find("p").text.strip() if category_block.find("p") else ""
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
        table = category_block.find_next("table")
        if not table:
            continue
        
        headers = [th.text.strip() for th in table.find_all("th")]
        rows = [[td.text.strip() for td in tr.find_all("td")] for tr in table.find_all("tr") if tr.find_all("td")]
        
        participants = []
        if "#" in headers and "–ò–º—è" in headers:
            for row in rows:
                if len(row) < 5 or not row[1] or not row[2]:
                    continue
                if row[0] == competition_name:
                    continue
                
                participants.append({
                    "place": row[0],
                    "name": row[1],
                    "region": row[2],
                    "start_time": row[3],
                    "score": row[4] if len(row) > 4 else ""
                })
        
        temp_categories.append({
            "name": raw_category,
            "time_range": time_range,
            "participants": participants
        })
    
    # –¢–µ–ø–µ—Ä—å –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –¥–ª—è –≤—Å–µ—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    for temp_cat in temp_categories:
        status = determine_category_status(
            temp_cat["time_range"], 
            current_time, 
            temp_cat["participants"], 
            temp_cat["name"], 
            temp_categories
        )
        
        categories.append({
            "name": temp_cat["name"],
            "time_range": temp_cat["time_range"],
            "status": status,
            "participants": temp_cat["participants"]
        })
    
    return {
        "name": competition_name,
        "regulation": regulation,
        "categories": categories
    }


def determine_category_status(time_range, current_time, participants, category_name, all_categories):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    if not participants:
        return "future"  # –ù–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ - –∫–∞—Ç–µ–≥–æ—Ä–∏—è –µ—â–µ –Ω–µ –Ω–∞—á–∞–ª–∞—Å—å
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ª–∏ —É—á–∞—Å—Ç–Ω–∏–∫–∏ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ü–µ–Ω–∫–∏ (–æ—Ü–µ–Ω–∫–∞ "-" –æ–∑–Ω–∞—á–∞–µ—Ç —á—Ç–æ –µ—â–µ –Ω–µ –ø—Ä–æ—à–ª–æ)
    participants_with_mark = [p for p in participants 
                             if p.get("score") 
                             and p.get("score").strip() != "" 
                             and p.get("score").strip() != "-"]
    
    if len(participants_with_mark) == len(participants):
        return "past"  # –í—Å–µ –ø–æ–ª—É—á–∏–ª–∏ –æ—Ü–µ–Ω–∫–∏ - –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∫–æ–≤—Ä–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    carpet_number = extract_carpet_number(category_name)
    
    if carpet_number:
        # –ò—â–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ —ç—Ç–æ–º –∂–µ –∫–æ–≤—Ä–µ
        previous_categories = [cat for cat in all_categories 
                              if extract_carpet_number(cat.get("name", "")) == carpet_number 
                              and cat.get("name", "") != category_name]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        previous_completed = True
        for prev_cat in previous_categories:
            prev_participants = prev_cat.get("participants", [])
            prev_with_marks = [p for p in prev_participants 
                              if p.get("score") 
                              and p.get("score").strip() != "" 
                              and p.get("score").strip() != "-"]
            if len(prev_with_marks) < len(prev_participants):
                previous_completed = False
                break
        
        if not previous_completed:
            return "future"  # –ü—Ä–µ–¥—ã–¥—É—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –µ—â–µ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞
        
        # –ï—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, —ç—Ç–∞ –∏–¥–µ—Ç
        return "current"
    
    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–æ–º–µ—Ä –∫–æ–≤—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É
    if participants_with_mark:
        return "current"  # –ï—Å—Ç—å –æ—Ü–µ–Ω–∫–∏ - –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∏–¥–µ—Ç
    else:
        return "future"   # –ù–µ—Ç –æ—Ü–µ–Ω–æ–∫ - –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Å–∫–æ—Ä–æ


def extract_carpet_number(category_name):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –∫–æ–≤—Ä–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    import re
    # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "–ö–æ–≤–µ—Ä 1", "–ö–æ–≤–µ—Ä 2", "–ö1", "–ö2" –∏ —Ç.–¥.
    patterns = [
        r'–ö–æ–≤–µ—Ä\s*(\d+)',
        r'–ö(\d+)',
        r'Carpet\s*(\d+)',
        r'C(\d+)'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, category_name, re.IGNORECASE)
        if match:
            return int(match.group(1))
    
    return None


def split_category_name(raw_name):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–∞ –∫–æ–≤–µ—Ä –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
    parts = raw_name.split("–ö–æ–≤–µ—Ä")
    if len(parts) > 1:
        mat = parts[0].strip() + "–ö–æ–≤–µ—Ä"
        category_name = parts[1].strip()
        return mat, category_name
    return "", raw_name


def parse_competition_results(start_id, end_id):
    results = []
 
    for comp_id in range(start_id, end_id + 1):
        url = f"{BASE_URL}{comp_id}"
        print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞: {url}")
        html = fetch_page(url)
        if not html:
            continue
 
        soup = BeautifulSoup(html, "html.parser")
        competition_name = soup.find("title").text.strip().split(" | ")[-1] if soup.find("title") else f"comp_{comp_id}"
 
        for category_block in soup.find_all("div", class_="d-flex"):
            category_name_tag = category_block.find("h3")
            if not category_name_tag:
                continue
 
            raw_category = category_name_tag.text.strip().replace("\n", " ")
            mat, category_name = split_category_name(raw_category)
            time_range = category_block.find("p").text.strip() if category_block.find("p") else ""
 
            table = category_block.find_next("table")
            if not table:
                continue
 
            headers = [th.text.strip() for th in table.find_all("th")]
            rows = [[td.text.strip() for td in tr.find_all("td")] for tr in table.find_all("tr") if tr.find_all("td")]
 
            if "#" in headers and "–ò–º—è" in headers:
                for row in rows:
                    if len(row) < 5 or not row[1] or not row[2]:
                        continue
 
                    if row[0] == competition_name:
                        continue
 
                    results.append({
                        "competition id": comp_id,
                        "competition name": competition_name,
                        "mat": mat,
                        "category name": category_name,
                        "place": row[0] if row else "",
                        "name": row[1] if len(row) > 1 else "",
                        "region": row[2] if len(row) > 2 else "",
                        "start time": row[3] if len(row) > 3 else "",
                        "score": row[4] if len(row) > 4 else "",
                    })
        time.sleep(SLEEP_TIME)
 
    df = pd.DataFrame(results)
    return df
 
 
 
 
def sync_all_data(request):
    """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ: —Å–ø–∏—Å–æ–∫ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –∏ –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
    print("=== –ù–∞—á–∞–ª–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö ===")
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π
    print("1. –ü–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π...")
    competitions = parse_competitions()
    write_competitions(competitions)
    print(f"–°–ø–∏—Å–æ–∫ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω: {len(competitions)} —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π")
    
    # 2. –°–∫–∞—á–∏–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–∂–¥–æ–º —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–∏
    print("2. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è—Ö...")
    from ..models import Competition
    
    all_db_competitions = Competition.objects.all()
    details_count = 0
    
    for comp in all_db_competitions:
        if comp.link:
            print(f"  - –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {comp.name}")
            detail_data = parse_competition_detail(comp.link)
            if detail_data:
                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ë–î
                # –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∫–∞–∫ JSON –ø–æ–ª–µ
                details_count += 1
                print(f"    ‚úì –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞")
            else:
                print(f"    ‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é")
        else:
            print(f"  - –ü—Ä–æ–ø—É—Å–∫ (–Ω–µ—Ç —Å—Å—ã–ª–∫–∏): {comp.name}")
    
    print(f"–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–∫–∞—á–∞–Ω–∞ –¥–ª—è {details_count} —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π")
    print("=== –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ===")
    print("Competitions written")